#########################################################
#    Imports / Env setup                                #
#########################################################

import os
import random
import time
import logging
from datetime import datetime
import argparse


if 'TF_USE_LEGACY_KERAS' in os.environ:
    del os.environ['TF_USE_LEGACY_KERAS']

import flwr as fl

import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.metrics import AUC, Precision, Recall
from tensorflow.keras.losses import LogCosh
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import f1_score, classification_report
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
from collections import Counter
import pandas as pd
import matplotlib.pyplot as plt
from numpy import expand_dims

# import math
# import glob

# from tqdm import tqdm

# import seaborn as sns

# import pickle
# import joblib

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, LabelEncoder, MinMaxScaler
from sklearn.utils import shuffle

################################################################################################################
#                                               FL-GAN TRAINING Setup                                         #
################################################################################################################

class CentralACGan:
    def __init__(self, discriminator, generator, nids, x_train, x_val, y_train, y_val, x_test, y_test, BATCH_SIZE,
                 noise_dim, latent_dim, num_classes, input_dim, epochs, steps_per_epoch, learning_rate,
                 log_file="training.log"):
        # -- models
        self.generator = generator
        self.discriminator = discriminator
        self.nids = nids

        # -- I/O Specs for models
        self.batch_size = BATCH_SIZE
        self.noise_dim = noise_dim
        self.latent_dim = latent_dim
        self.num_classes = num_classes
        self.input_dim = input_dim

        # -- training duration
        self.max_epochs = epochs  # Store the maximum possible epochs
        self.epochs = min(25, epochs)  # Start with 25 epochs for progressive training
        self.steps_per_epoch = steps_per_epoch
        self.disc_gen_ratio = 2

        # -- Early stopping parameters
        self.early_stop_patience = 12  # Number of epochs to wait for improvement
        self.min_delta = 0.001  # Minimum change to count as improvement

        # -- Best model tracking
        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0
        self.best_disc_weights = None
        self.best_gen_weights = None
        self.patience_counter = 0
        self.best_epoch = 0

        # -- Data
        # Features
        self.x_train = x_train
        self.x_test = x_test
        self.x_val = x_val
        # Categorical Labels
        self.y_train = y_train
        self.y_test = y_test
        self.y_val = y_val

        # -- Setup Logging
        self.setup_logger(log_file)

        # -- Optimizers
        # LR decay
        lr_schedule_gen = tf.keras.optimizers.schedules.ExponentialDecay(
            initial_learning_rate=0.00001, decay_steps=10000, decay_rate=0.97, staircase=False)

        lr_schedule_disc = tf.keras.optimizers.schedules.ExponentialDecay(
            initial_learning_rate=0.00003, decay_steps=10000, decay_rate=0.97, staircase=False)

        # Compile optimizer
        self.gen_optimizer = Adam(learning_rate=lr_schedule_gen, beta_1=0.5, beta_2=0.999)
        self.disc_optimizer = Adam(learning_rate=lr_schedule_disc, beta_1=0.5, beta_2=0.999)

        print("Discriminator Output:", self.discriminator.output_names)

        # -- Model Compilations
        # Compile Discriminator separately (before freezing)
        # self.discriminator.compile(
        #     loss={'validity': 'binary_crossentropy', 'class': 'categorical_crossentropy'},
        #     optimizer=self.disc_optimizer,
        #     metrics={
        #         'validity': ['binary_accuracy'],
        #         'class': ['categorical_accuracy']
        #     }
        # )

        # Freeze Discriminator only for AC-GAN training
        self.discriminator.trainable = False

        # Define AC-GAN (Generator + Frozen Discriminator)
        # I/O
        noise_input = tf.keras.Input(shape=(latent_dim,))
        label_input = tf.keras.Input(shape=(1,), dtype='int32')
        generated_data = self.generator([noise_input, label_input])
        validity, class_pred = self.discriminator(generated_data)
        # Compile Combined Model
        self.ACGAN = Model([noise_input, label_input], [validity, class_pred])

        print("ACGAN Output:", self.ACGAN.output_names)

        self.ACGAN.compile(
            loss={'Discriminator': 'binary_crossentropy', 'Discriminator_1': 'categorical_crossentropy'},
            optimizer=self.gen_optimizer,
            metrics={
                'Discriminator': ['binary_accuracy'],
                'Discriminator_1': ['categorical_accuracy']
            }
        )

    # Saving function for ACGAN
    def setACGAN(self):
        return self.ACGAN

    # -- logging Functions -- #

    def setup_logger(self, log_file):
        """Set up a logger that records both to a file and to the console."""
        self.logger = logging.getLogger("CentralACGan")
        self.logger.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

        # File handler
        fh = logging.FileHandler(log_file)
        fh.setLevel(logging.INFO)
        fh.setFormatter(formatter)

        # Console handler
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)
        ch.setFormatter(formatter)

        # Avoid adding duplicate handlers if logger already has them.
        if not self.logger.handlers:
            self.logger.addHandler(fh)
            self.logger.addHandler(ch)

    def log_model_settings(self):
        """Logs model names, structures, and hyperparameters."""
        self.logger.info("=== Model Settings ===")

        self.logger.info("Generator Model Summary:")
        generator_summary = []
        self.generator.summary(print_fn=lambda x: generator_summary.append(x))
        for line in generator_summary:
            self.logger.info(line)

        self.logger.info("Discriminator Model Summary:")
        discriminator_summary = []
        self.discriminator.summary(print_fn=lambda x: discriminator_summary.append(x))
        for line in discriminator_summary:
            self.logger.info(line)

        if self.nids is not None:
            self.logger.info("NIDS Model Summary:")
            nids_summary = []
            self.nids.summary(print_fn=lambda x: nids_summary.append(x))
            for line in nids_summary:
                self.logger.info(line)
        else:
            self.logger.info("NIDS Model is not defined.")

        self.logger.info("--- Hyperparameters ---")
        self.logger.info(f"Batch Size: {self.batch_size}")
        self.logger.info(f"Noise Dimension: {self.noise_dim}")
        self.logger.info(f"Latent Dimension: {self.latent_dim}")
        self.logger.info(f"Number of Classes: {self.num_classes}")
        self.logger.info(f"Input Dimension: {self.input_dim}")
        self.logger.info(f"Epochs: {self.epochs}")
        self.logger.info(f"Steps per Epoch: {self.steps_per_epoch}")
        self.logger.info(f"Learning Rate (Generator): {self.gen_optimizer.learning_rate}")
        self.logger.info(f"Learning Rate (Discriminator): {self.disc_optimizer.learning_rate}")
        self.logger.info("=" * 50)

    def log_epoch_metrics(self, epoch, d_metrics, g_metrics, nids_metrics=None, fusion_metrics=None):
        """Logs a formatted summary of the metrics for this epoch."""
        self.logger.info(f"=== Epoch {epoch} Metrics Summary ===")
        self.logger.info("Discriminator Metrics:")
        for key, value in d_metrics.items():
            self.logger.info(f"  {key}: {value}")
        self.logger.info("Generator Metrics:")
        for key, value in g_metrics.items():
            self.logger.info(f"  {key}: {value}")
        if nids_metrics is not None:
            self.logger.info("NIDS Metrics:")
            for key, value in nids_metrics.items():
                self.logger.info(f"  {key}: {value}")
        if fusion_metrics is not None:
            self.logger.info("Probabilistic Fusion Metrics:")
            for key, value in fusion_metrics.items():
                self.logger.info(f"  {key}: {value}")
        self.logger.info("=" * 50)

    def log_evaluation_metrics(self, d_eval, g_eval, nids_eval=None, fusion_eval=None):
        """Logs a formatted summary of evaluation metrics."""
        self.logger.info("=== Evaluation Metrics Summary ===")
        self.logger.info("Discriminator Evaluation:")
        for key, value in d_eval.items():
            self.logger.info(f"  {key}: {value}")
        self.logger.info("Generator Evaluation:")
        for key, value in g_eval.items():
            self.logger.info(f"  {key}: {value}")
        if nids_eval is not None:
            self.logger.info("NIDS Evaluation:")
            for key, value in nids_eval.items():
                self.logger.info(f"  {key}: {value}")
        if fusion_eval is not None:
            self.logger.info("Probabilistic Fusion Evaluation:")
            for key, value in fusion_eval.items():
                self.logger.info(f"  {key}: {value}")
        self.logger.info("=" * 50)

    # -- Train -- #
    def fit(self, X_train=None, y_train=None, checkpoint_dir="../pretrainedModels/checkpoints/"):
        """
        Train the AC-GAN model with early stopping and progressive training.

        Parameters:
        -----------
        X_train : array-like, optional
            Training data features. If None, uses self.x_train.
        y_train : array-like, optional
            Training data labels. If None, uses self.y_train.
        checkpoint_dir : str, default="../pretrainedModels/checkpoints/"
            Directory to save model checkpoints during training.
        """
        if X_train is None or y_train is None:
            X_train = self.x_train
            y_train = self.y_train

        # Create checkpoint directory if it doesn't exist
        os.makedirs(checkpoint_dir, exist_ok=True)

        # -- make sure discriminator is trainable for individual training -- #
        self.discriminator.trainable = True
        # Ensure all layers within discriminator are trainable
        for layer in self.discriminator.layers:
            layer.trainable = True

        # -- Re-compile discriminator with trainable weights -- #
        self.discriminator.compile(
            loss={'validity': 'binary_crossentropy', 'class': 'categorical_crossentropy'},
            optimizer=self.disc_optimizer,
            metrics={
                'validity': ['binary_accuracy'],
                'class': ['categorical_accuracy']
            }
        )

        # Log model settings at the start
        self.log_model_settings()

        # -- Apply label smoothing -- #
        valid_smoothing_factor = 0.15
        valid_smooth = tf.ones((self.batch_size, 1)) * (1 - valid_smoothing_factor)

        fake_smoothing_factor = 0.1
        fake_smooth = tf.zeros((self.batch_size, 1)) + fake_smoothing_factor

        # For generator training, we use a slightly different smoothing
        # to keep the generator from becoming too confident
        gen_smoothing_factor = 0.1
        valid_smooth_gen = tf.ones((self.batch_size, 1)) * (1 - gen_smoothing_factor)  # Slightly less than 1.0

        self.logger.info(f"Using valid label smoothing with factor: {valid_smoothing_factor}")
        self.logger.info(f"Using fake label smoothing with factor: {fake_smoothing_factor}")
        self.logger.info(f"Using gen label smoothing with factor: {gen_smoothing_factor}")

        # Early stopping and progressive training setup
        self.logger.info(
            f"Initial training phase: {self.epochs} epochs with early stopping patience: {self.early_stop_patience}")
        self.logger.info(f"Maximum possible epochs: {self.max_epochs}")

        # Reset early stopping tracking variables
        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0
        self.patience_counter = 0
        self.best_epoch = 0
        self.best_disc_weights = None
        self.best_gen_weights = None

        # -- Training Loop -- #
        total_epochs_trained = 0
        training_phase = 1

        while total_epochs_trained < self.max_epochs:
            remaining_epochs = self.max_epochs - total_epochs_trained
            phase_epochs = min(self.epochs, remaining_epochs)

            if phase_epochs <= 0:
                break

            self.logger.info(f"\n=== Starting Training Phase {training_phase} ===")
            self.logger.info(f"Epochs for this phase: {phase_epochs}")
            self.logger.info(f"Total epochs trained so far: {total_epochs_trained}")
            self.logger.info(f"Maximum epochs: {self.max_epochs}")

            # Train for this phase
            for epoch in range(phase_epochs):
                current_epoch = total_epochs_trained + epoch
                print(f'\n=== Epoch {current_epoch + 1}/{self.max_epochs} (Phase {training_phase}) ===\n')
                self.logger.info(f'=== Epoch {current_epoch}/{self.max_epochs} (Phase {training_phase}) ===')

                # Track metrics for logging
                d_loss_avg = 0
                d_validity_acc_avg = 0
                d_class_acc_avg = 0

                # --------------------------
                # Train Discriminator
                # --------------------------
                for d_iter in range(self.disc_gen_ratio):
                    # -- Source the real data -- #
                    idx = tf.random.shuffle(tf.range(len(X_train)))[:self.batch_size]
                    real_data = tf.gather(X_train, idx)
                    real_labels = tf.gather(y_train, idx)

                    # Ensure labels are one-hot encoded
                    if len(real_labels.shape) == 1:
                        real_labels_onehot = tf.one_hot(tf.cast(real_labels, tf.int32), depth=self.num_classes)
                    else:
                        real_labels_onehot = real_labels

                    # -- Generate fake data -- #
                    noise = tf.random.normal((self.batch_size, self.latent_dim))
                    fake_labels = tf.random.uniform((self.batch_size,), minval=0, maxval=self.num_classes,
                                                    dtype=tf.int32)
                    fake_labels_onehot = tf.one_hot(fake_labels, depth=self.num_classes)

                    # Generate data from noise and desired labels
                    generated_data = self.generator.predict([noise, fake_labels])

                    # -- Train discriminator on real and fake data -- #
                    d_loss_real = self.discriminator.train_on_batch(real_data, [valid_smooth, real_labels_onehot])
                    d_loss_fake = self.discriminator.train_on_batch(generated_data, [fake_smooth, fake_labels_onehot])
                    d_loss = 0.5 * tf.add(d_loss_real, d_loss_fake)

                    # Accumulate metrics for averaging
                    d_loss_avg += d_loss[0]
                    d_validity_acc_avg += d_loss[3]
                    d_class_acc_avg += d_loss[4]

                    # Log the last iteration metrics
                    if d_iter == self.disc_gen_ratio - 1:
                        # Average the metrics over all iterations
                        d_loss_avg /= self.disc_gen_ratio
                        d_validity_acc_avg /= self.disc_gen_ratio
                        d_class_acc_avg /= self.disc_gen_ratio

                        # Collect discriminator metrics
                        d_metrics = {
                            "Total Loss": f"{d_loss_avg:.4f}",
                            "Validity Loss": f"{d_loss[1]:.4f}",
                            "Class Loss": f"{d_loss[2]:.4f}",
                            "Validity Binary Accuracy": f"{d_validity_acc_avg * 100:.2f}%",
                            "Class Categorical Accuracy": f"{d_class_acc_avg * 100:.2f}%"
                        }
                        self.logger.info(f"Training Discriminator ({self.disc_gen_ratio} iterations)")
                        self.logger.info(
                            f"Discriminator Avg Total Loss: {d_loss_avg:.4f} | Last Valid Loss: {d_loss[1]:.4f} | Last Class Loss: {d_loss[2]:.4f}")
                        self.logger.info(
                            f"Avg Validity Binary Accuracy: {d_validity_acc_avg * 100:.2f}%")
                        self.logger.info(
                            f"Avg Class Categorical Accuracy: {d_class_acc_avg * 100:.2f}%")
                # --------------------------
                # Train Generator (AC-GAN)
                # --------------------------
                # -- Generate noise and label inputs for ACGAN -- #
                noise = tf.random.normal((self.batch_size, self.latent_dim))
                sampled_labels = tf.random.uniform((self.batch_size,), minval=0, maxval=self.num_classes,
                                                   dtype=tf.int32)
                sampled_labels_onehot = tf.one_hot(sampled_labels, depth=self.num_classes)

                # -- Train ACGAN with sampled noise data -- #
                g_loss = self.ACGAN.train_on_batch([noise, sampled_labels], [valid_smooth_gen, sampled_labels_onehot])

                # Collect generator metrics
                g_metrics = {
                    "Total Loss": f"{g_loss[0]:.4f}",
                    "Validity Loss": f"{g_loss[1]:.4f}",  # This is Discriminator_loss
                    "Class Loss": f"{g_loss[2]:.4f}",  # This is Discriminator_1_loss
                    "Validity Binary Accuracy": f"{g_loss[3] * 100:.2f}%",  # Discriminator_binary_accuracy
                    "Class Categorical Accuracy": f"{g_loss[4] * 100:.2f}%"  # Discriminator_1_categorical_accuracy
                }
                self.logger.info("Training Generator with ACGAN FLOW")
                self.logger.info(
                    f"AC-GAN Generator Total Loss: {g_loss[0]:.4f} | Validity Loss: {g_loss[1]:.4f} | Class Loss: {g_loss[2]:.4f}")
                self.logger.info(
                    f"Validity Binary Accuracy: {g_loss[3] * 100:.2f}%")
                self.logger.info(
                    f"Class Categorical Accuracy: {g_loss[4] * 100:.2f}%")

                # --------------------------
                # Validation & Early Stopping
                # --------------------------
                # -- GAN Validation --#
                d_val_loss, d_val_metrics = self.validation_disc()
                g_val_loss, g_val_metrics = self.validation_gen()

                # -- Probabilistic Fusion Validation -- #
                self.logger.info("=== Probabilistic Fusion Validation on Real Data ===")
                fusion_results, fusion_metrics = self.validate_with_probabilistic_fusion(self.x_val, self.y_val)
                fusion_accuracy = fusion_metrics['accuracy']
                self.logger.info(f"Probabilistic Fusion Accuracy: {fusion_accuracy * 100:.2f}%")

                # Log distribution of classifications
                self.logger.info(f"Predicted Class Distribution: {fusion_metrics['predicted_class_distribution']}")
                self.logger.info(f"Correct Class Distribution: {fusion_metrics['correct_class_distribution']}")
                self.logger.info(f"True Class Distribution: {fusion_metrics['true_class_distribution']}")

                # -- NIDS Validation -- #
                nids_val_metrics = None
                if self.nids is not None:
                    nids_custom_loss, nids_val_metrics = self.validation_NIDS()
                    self.logger.info(f"Validation NIDS Custom Loss: {nids_custom_loss:.4f}")

                # -- Log the metrics for epoch -- #
                self.log_epoch_metrics(current_epoch, d_val_metrics, g_val_metrics, nids_val_metrics, fusion_metrics)

                # -- Check if this is the best model so far -- #
                # We use a combined metric: discriminator validation accuracy + fusion accuracy
                # Higher is better
                combined_disc_val_accuracy = float(d_val_metrics["Real Validity Binary Accuracy"].rstrip('%')) / 100
                combined_metric = combined_disc_val_accuracy + fusion_accuracy

                if combined_metric > self.best_val_acc:
                    # Record new best
                    self.best_val_acc = combined_metric
                    self.best_epoch = current_epoch
                    self.patience_counter = 0
                    self.logger.info(f"New best model found! Combined accuracy: {combined_metric:.4f}")

                    # Save best weights
                    self.best_disc_weights = self.discriminator.get_weights()
                    self.best_gen_weights = self.generator.get_weights()

                    # Save checkpoint
                    checkpoint_path = os.path.join(checkpoint_dir,
                                                   f"best_models_phase{training_phase}_epoch{current_epoch}")
                    os.makedirs(checkpoint_path, exist_ok=True)
                    self.discriminator.save(os.path.join(checkpoint_path, "discriminator.h5"))
                    self.generator.save(os.path.join(checkpoint_path, "generator.h5"))
                    self.logger.info(f"Saved best models to {checkpoint_path}")
                else:
                    # Increment patience counter
                    self.patience_counter += 1
                    self.logger.info(f"No improvement. Patience: {self.patience_counter}/{self.early_stop_patience}")

                    # Check for early stopping
                    if self.patience_counter >= self.early_stop_patience:
                        self.logger.info(f"Early stopping triggered after {current_epoch + 1} epochs")
                        self.logger.info(
                            f"Best model was at epoch {self.best_epoch} with combined accuracy: {self.best_val_acc:.4f}")

                        # Restore best weights
                        if self.best_disc_weights is not None and self.best_gen_weights is not None:
                            self.discriminator.set_weights(self.best_disc_weights)
                            self.generator.set_weights(self.best_gen_weights)
                            self.logger.info("Restored weights from best epoch")

                        break  # Exit the epoch loop early

            # Update total epochs trained
            total_epochs_trained += epoch + 1

            # Check if we've reached the maximum epochs
            if total_epochs_trained >= self.max_epochs:
                self.logger.info(f"Reached maximum epochs ({self.max_epochs}). Training complete.")
                break

            # Check if early stopping was triggered
            if self.patience_counter >= self.early_stop_patience:
                # Progressive training: decide whether to continue
                remaining_epochs = self.max_epochs - total_epochs_trained

                if remaining_epochs > 0:
                    # Reset patience counter for next phase
                    self.patience_counter = 0

                    # Evaluate if metrics are still good enough to continue
                    if self.best_val_acc > 0.75:  # Example threshold - continue if accuracy is good
                        self.logger.info(
                            f"Performance still improving (acc={self.best_val_acc:.4f}). Starting next training phase.")
                        self.epochs = min(25, remaining_epochs)  # Next block of max 25 epochs
                        training_phase += 1
                    else:
                        self.logger.info(
                            f"Performance plateau reached (acc={self.best_val_acc:.4f}). Stopping training.")
                        break
                else:
                    self.logger.info("No remaining epochs available. Training complete.")
                    break
            else:
                # If we completed all epochs in this phase without early stopping
                remaining_epochs = self.max_epochs - total_epochs_trained

                if remaining_epochs > 0:
                    self.logger.info(f"Completed phase {training_phase} without early stopping. Starting next phase.")
                    self.epochs = min(25, remaining_epochs)
                    training_phase += 1
                else:
                    self.logger.info("Maximum epochs reached. Training complete.")
                    break

        # Final summary
        self.logger.info("\n=== Training Summary ===")
        self.logger.info(f"Total epochs trained: {total_epochs_trained} out of maximum {self.max_epochs}")
        self.logger.info(f"Best model found at epoch {self.best_epoch} with combined accuracy: {self.best_val_acc:.4f}")

        # Restore best model if needed
        if total_epochs_trained > 0 and self.best_disc_weights is not None and self.best_gen_weights is not None:
            self.discriminator.set_weights(self.best_disc_weights)
            self.generator.set_weights(self.best_gen_weights)
            self.logger.info("Final models are using weights from the best epoch")

        return {
            "epochs_trained": total_epochs_trained,
            "best_epoch": self.best_epoch,
            "best_accuracy": self.best_val_acc,
            "phases_completed": training_phase
        }
        # -- Loss Calculation -- #

    def nids_loss(self, real_output, fake_output):
        """
        Compute the NIDS loss on real and fake samples.
        For real samples, the target is 1 (benign), and for fake samples, 0 (attack).
        Returns a scalar loss value.
        """
        # define labels
        real_labels = tf.ones_like(real_output)
        fake_labels = tf.zeros_like(fake_output)

        # define loss function
        bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)

        # calculate outputs
        real_loss = bce(real_labels, real_output)
        fake_loss = bce(fake_labels, fake_output)

        # sum up total loss
        total_loss = real_loss + fake_loss
        return total_loss.numpy()

    # -- Probabilistic Fusion Methods -- #
    def probabilistic_fusion(self, input_data):
        """
        Apply probabilistic fusion to combine validity and class predictions.
        Returns combined probabilities for all four possible outcomes.
        """
        # Get discriminator predictions
        validity_scores, class_predictions = self.discriminator.predict(input_data)

        total_samples = len(input_data)
        results = []

        for i in range(total_samples):
            # Validity probabilities: P(valid) and P(invalid)
            p_valid = validity_scores[i][0]  # Probability of being valid/real
            p_invalid = 1 - p_valid  # Probability of being invalid/fake

            # Class probabilities: 2 classes (benign=0, attack=1)
            p_benign = class_predictions[i][0]  # Probability of being benign
            p_attack = class_predictions[i][1]  # Probability of being attack

            # Calculate joint probabilities for all combinations
            p_valid_benign = p_valid * p_benign
            p_valid_attack = p_valid * p_attack
            p_invalid_benign = p_invalid * p_benign
            p_invalid_attack = p_invalid * p_attack

            # Store all probabilities in a dictionary
            probabilities = {
                "valid_benign": p_valid_benign,
                "valid_attack": p_valid_attack,
                "invalid_benign": p_invalid_benign,
                "invalid_attack": p_invalid_attack
            }

            # Find the most likely outcome
            most_likely = max(probabilities, key=probabilities.get)

            # For analysis, add the actual probabilities alongside the classification
            result = {
                "classification": most_likely,
                "probabilities": probabilities
            }

            results.append(result)

        return results

    def validate_with_probabilistic_fusion(self, validation_data, validation_labels=None):
        """
        Evaluate model using probabilistic fusion and calculate metrics if labels are available.
        """
        fusion_results = self.probabilistic_fusion(validation_data)

        # Extract classifications
        classifications = [result["classification"] for result in fusion_results]

        # Count occurrences of each class
        predicted_class_distribution = Counter(classifications)
        self.logger.info(f"Predicted Class Distribution: {dict(predicted_class_distribution)}")

        # If we have ground truth labels, calculate accuracy
        if validation_labels is not None:
            correct_predictions = 0
            correct_classifications = []
            true_classifications = []

            for i, result in enumerate(fusion_results):
                # Get the true label (assuming 0=benign, 1=attack)
                if isinstance(validation_labels, np.ndarray) and validation_labels.ndim > 1:
                    true_class_idx = np.argmax(validation_labels[i])
                else:
                    true_class_idx = validation_labels[i]

                true_class = "benign" if true_class_idx == 0 else "attack"

                # For validation data (which is real), expected validity is "valid"
                true_validity = "valid"  # Since validation data is real data

                # Construct the true combined label
                true_combined = f"{true_validity}_{true_class}"

                # Add to true classifications list
                true_classifications.append(true_combined)

                # Check if prediction matches
                if result["classification"] == true_combined:
                    correct_predictions += 1
                    correct_classifications.append(result["classification"])

            # Count distribution of correctly classified samples
            correct_class_distribution = Counter(correct_classifications)

            # Count distribution of true classes
            true_class_distribution = Counter(true_classifications)
            self.logger.info(f"True Class Distribution: {dict(true_class_distribution)}")

            accuracy = correct_predictions / len(validation_data)
            self.logger.info(f"Accuracy: {accuracy:.4f}")

            metrics = {
                "accuracy": accuracy,
                "total_samples": len(validation_data),
                "correct_predictions": correct_predictions,
                "predicted_class_distribution": dict(predicted_class_distribution),
                "correct_class_distribution": dict(correct_class_distribution),
                "true_class_distribution": dict(true_class_distribution)
            }

            return classifications, metrics

        return classifications, {"predicted_class_distribution": dict(predicted_class_distribution)}

    def analyze_fusion_results(self, fusion_results):
        """Analyze the distribution of probabilities from fusion results"""
        # Extract probabilities for each category
        valid_benign_probs = [r["probabilities"]["valid_benign"] for r in fusion_results]
        valid_attack_probs = [r["probabilities"]["valid_attack"] for r in fusion_results]
        invalid_benign_probs = [r["probabilities"]["invalid_benign"] for r in fusion_results]
        invalid_attack_probs = [r["probabilities"]["invalid_attack"] for r in fusion_results]

        # Calculate summary statistics
        categories = ["Valid Benign", "Valid Attack", "Invalid Benign", "Invalid Attack"]
        all_probs = [valid_benign_probs, valid_attack_probs, invalid_benign_probs, invalid_attack_probs]

        for cat, probs in zip(categories, all_probs):
            self.logger.info(
                f"{cat}: Mean={np.mean(probs):.4f}, Median={np.median(probs):.4f}, Max={np.max(probs):.4f}")

        # You could add additional visualizations or analysis here

    # -- Validate -- #
    def validation_disc(self):
        """
        Evaluate the discriminator on the validation set.
        First, evaluate on real data (with labels = 1) and then on fake data (labels = 0).
        Prints and returns the average total loss along with a metrics dictionary.
        """
        # --- Evaluate on real validation data ---
        val_valid_labels = np.ones((len(self.x_val), 1))

        # Ensure y_val is one-hot encoded if needed
        if self.y_val.ndim == 1 or self.y_val.shape[1] != self.num_classes:
            y_val_onehot = tf.one_hot(self.y_val, depth=self.num_classes)
        else:
            y_val_onehot = self.y_val

        d_loss_real = self.discriminator.evaluate(
            self.x_val, [val_valid_labels, y_val_onehot], verbose=0
        )

        # --- Evaluate on generated (fake) data ---
        noise = tf.random.normal((len(self.x_val), self.latent_dim))
        fake_labels = tf.random.uniform(
            (len(self.x_val),), minval=0, maxval=self.num_classes, dtype=tf.int32
        )
        fake_labels_onehot = tf.one_hot(fake_labels, depth=self.num_classes)
        fake_valid_labels = np.zeros((len(self.x_val), 1))
        generated_data = self.generator.predict([noise, fake_labels])
        d_loss_fake = self.discriminator.evaluate(
            generated_data, [fake_valid_labels, fake_labels_onehot], verbose=0
        )

        # --- Compute average loss ---
        avg_total_loss = 0.5 * (d_loss_real[0] + d_loss_fake[0])

        self.logger.info("Validation Discriminator Evaluation:")
        # Log for real data: using all relevant indices
        self.logger.info(
            f"Real Data -> Total Loss: {d_loss_real[0]:.4f}, "
            f"Validity Loss: {d_loss_real[1]:.4f}, "
            f"Class Loss: {d_loss_real[2]:.4f}, "
            f"Validity Binary Accuracy: {d_loss_real[3] * 100:.2f}%, "
            f"Class Categorical Accuracy: {d_loss_real[4] * 100:.2f}%"
        )
        print("-----------")
        self.logger.info(
            f"Fake Data -> Total Loss: {d_loss_fake[0]:.4f}, "
            f"Validity Loss: {d_loss_fake[1]:.4f}, "
            f"Class Loss: {d_loss_fake[2]:.4f}, "
            f"Validity Binary Accuracy: {d_loss_fake[3] * 100:.2f}%, "
            f"Class Categorical Accuracy: {d_loss_fake[4] * 100:.2f}%"
        )
        self.logger.info(f"Average Discriminator Loss: {avg_total_loss:.4f}")

        metrics = {
            "Real Total Loss": f"{d_loss_real[0]:.4f}",
            "Real Validity Loss": f"{d_loss_real[1]:.4f}",
            "Real Class Loss": f"{d_loss_real[2]:.4f}",
            "Real Validity Binary Accuracy": f"{d_loss_real[3] * 100:.2f}%",
            "Real Class Categorical Accuracy": f"{d_loss_real[4] * 100:.2f}%",
            "Fake Total Loss": f"{d_loss_fake[0]:.4f}",
            "Fake Validity Loss": f"{d_loss_fake[1]:.4f}",
            "Fake Class Loss": f"{d_loss_fake[2]:.4f}",
            "Fake Validity Binary Accuracy": f"{d_loss_fake[3] * 100:.2f}%",
            "Fake Class Categorical Accuracy": f"{d_loss_fake[4] * 100:.2f}%",
            "Average Total Loss": f"{avg_total_loss:.4f}"
        }
        return avg_total_loss, metrics

    def validation_gen(self):
        """
        Evaluate the generator (via the AC-GAN) using a validation batch.
        The generator is evaluated by its ability to “fool” the discriminator.
        Prints and returns the total generator loss along with key metrics.
        """
        noise = tf.random.normal((len(self.x_val), self.latent_dim))
        sampled_labels = tf.random.uniform(
            (len(self.x_val),), minval=0, maxval=self.num_classes, dtype=tf.int32
        )
        sampled_labels_onehot = tf.one_hot(sampled_labels, depth=self.num_classes)
        valid_labels = np.ones((len(self.x_val), 1))

        g_loss = self.ACGAN.evaluate(
            [noise, sampled_labels],
            [valid_labels, sampled_labels_onehot],
            verbose=0
        )

        # Log detailed metrics
        self.logger.info("Validation Generator (AC-GAN) Evaluation:")
        self.logger.info(
            f"Total Loss: {g_loss[0]:.4f}, Validity Loss: {g_loss[1]:.4f}, Class Loss: {g_loss[2]:.4f}")
        self.logger.info(
            f"Validity Binary Accuracy: {g_loss[3] * 100:.2f}%")
        self.logger.info(
            f"Class Categorical Accuracy: {g_loss[4] * 100:.2f}%")

        # Create a metrics dictionary with the full set of metrics
        g_metrics = {
            "Total Loss": f"{g_loss[0]:.4f}",
            "Validity Loss": f"{g_loss[1]:.4f}",
            "Class Loss": f"{g_loss[2]:.4f}",
            "Validity Binary Accuracy": f"{g_loss[3] * 100:.2f}%",
            "Class Categorical Accuracy": f"{g_loss[4] * 100:.2f}%"
        }
        return g_loss[0], g_metrics

    def validation_NIDS(self):
        """
        Evaluate the NIDS model on validation data augmented with generated fake samples.
        Real data is labeled as 1 (benign) and fake/generated data as 0 (attack).
        Prints detailed metrics including a classification report and returns the custom
        NIDS loss along with a metrics dictionary.
        """
        if self.nids is None:
            print("NIDS model is not defined.")
            return None

        # --- Prepare Real Data ---
        X_real = self.x_val
        y_real = np.ones((len(self.x_val),), dtype="int32")  # Real samples labeled 1

        # --- Generate Fake Data ---
        noise = tf.random.normal((len(self.x_val), self.latent_dim))
        fake_labels = tf.random.uniform(
            (len(self.x_val),), minval=0, maxval=self.num_classes, dtype=tf.int32
        )
        generated_samples = self.generator.predict([noise, fake_labels])
        X_fake = generated_samples
        y_fake = np.zeros((len(self.x_val),), dtype="int32")  # Fake samples labeled 0

        # --- Compute custom NIDS loss ---
        real_output = self.nids.predict(X_real)
        fake_output = self.nids.predict(X_fake)
        custom_nids_loss = self.nids_loss(real_output, fake_output)

        # --- Evaluate on the Combined Dataset ---
        X_combined = np.vstack([X_real, X_fake])
        y_combined = np.hstack([y_real, y_fake])
        nids_eval = self.nids.evaluate(X_combined, y_combined, verbose=0)
        # Expected order: [loss, accuracy, precision, recall, auc, logcosh]

        # --- Compute Additional Metrics ---
        y_pred_probs = self.nids.predict(X_combined)
        y_pred = (y_pred_probs > 0.5).astype("int32")
        f1 = f1_score(y_combined, y_pred)
        class_report = classification_report(
            y_combined, y_pred, target_names=["Attack (Fake)", "Benign (Real)"]
        )

        self.logger.info("Validation NIDS Evaluation with Augmented Data:")
        self.logger.info(f"Custom NIDS Loss (Real vs Fake): {custom_nids_loss:.4f}")
        self.logger.info(f"Overall NIDS Loss: {nids_eval[0]:.4f}, Accuracy: {nids_eval[1]:.4f}, "
                         f"Precision: {nids_eval[2]:.4f}, Recall: {nids_eval[3]:.4f}, "
                         f"AUC: {nids_eval[4]:.4f}, LogCosh: {nids_eval[5]:.4f}")
        self.logger.info("Classification Report:")
        self.logger.info(class_report)
        self.logger.info(f"F1 Score: {f1:.4f}")

        metrics = {
            "Custom NIDS Loss": f"{custom_nids_loss:.4f}",
            "Loss": f"{nids_eval[0]:.4f}",
            "Accuracy": f"{nids_eval[1]:.4f}",
            "Precision": f"{nids_eval[2]:.4f}",
            "Recall": f"{nids_eval[3]:.4f}",
            "AUC": f"{nids_eval[4]:.4f}",
            "LogCosh": f"{nids_eval[5]:.4f}",
            "F1 Score": f"{f1:.4f}"
        }
        return custom_nids_loss, metrics

    # -- Evaluate -- #
    def evaluate(self, X_test=None, y_test=None):
        if X_test is None or y_test is None:
            X_test = self.x_test
            y_test = self.y_test

        # --------------------------
        # Test Discriminator
        # --------------------------
        self.logger.info("-- Evaluating Discriminator --")
        # run the model
        results = self.discriminator.evaluate(X_test, [tf.ones((len(y_test), 1)), y_test], verbose=0)
        # Using the updated ordering:
        d_loss_total = results[0]
        d_loss_validity = results[1]
        d_loss_class = results[2]
        d_validity_bin_acc = results[3]
        d_class_cat_acc = results[4]

        d_eval_metrics = {
            "Loss": f"{d_loss_total:.4f}",
            "Validity Loss": f"{d_loss_validity:.4f}",
            "Class Loss": f"{d_loss_class:.4f}",
            "Validity Binary Accuracy": f"{d_validity_bin_acc * 100:.2f}%",
            "Class Categorical Accuracy": f"{d_class_cat_acc * 100:.2f}%"
        }
        self.logger.info(
            f"Discriminator Total Loss: {d_loss_total:.4f} | Validity Loss: {d_loss_validity:.4f} | Class Loss: {d_loss_class:.4f}"
        )
        self.logger.info(
            f"Validity Binary Accuracy: {d_validity_bin_acc * 100:.2f}%"
        )
        self.logger.info(
            f"Class Categorical Accuracy: {d_class_cat_acc * 100:.2f}%"
        )

        # --------------------------
        # Test Generator (ACGAN)
        # --------------------------
        self.logger.info("-- Evaluating Generator --")

        # get the noise samples
        noise = tf.random.normal((len(y_test), self.latent_dim))
        sampled_labels = tf.random.uniform((len(y_test),), minval=0, maxval=self.num_classes, dtype=tf.int32)

        # run the model
        g_loss = self.ACGAN.evaluate([noise, sampled_labels],
                                     [tf.ones((len(y_test), 1)),
                                      tf.one_hot(sampled_labels, depth=self.num_classes)],
                                     verbose=0)

        # Using the updated ordering for ACGAN:
        g_loss_total = g_loss[0]
        g_loss_validity = g_loss[1]
        g_loss_class = g_loss[2]
        g_validity_bin_acc = g_loss[3]
        g_class_cat_acc = g_loss[4]

        g_eval_metrics = {
            "Loss": f"{g_loss_total:.4f}",
            "Validity Loss": f"{g_loss_validity:.4f}",
            "Class Loss": f"{g_loss_class:.4f}",
            "Validity Binary Accuracy": f"{g_validity_bin_acc * 100:.2f}%",
            "Class Categorical Accuracy": f"{g_class_cat_acc * 100:.2f}%"
        }
        self.logger.info(
            f"Generator Total Loss: {g_loss_total:.4f} | Validity Loss: {g_loss_validity:.4f} | Class Loss: {g_loss_class:.4f}"
        )
        self.logger.info(
            f"Validity Binary Accuracy: {g_validity_bin_acc * 100:.2f}%"
        )
        self.logger.info(
            f"Class Categorical Accuracy: {g_class_cat_acc * 100:.2f}%"
        )

        # --------------------------
        # Test NIDS
        # --------------------------
        nids_eval_metrics = None
        if self.nids is not None:
            self.logger.info("-- Evaluating NIDS --")
            # Prepare real test data (labeled as benign, 1)
            X_real = X_test
            y_real = np.ones((len(X_test),), dtype="int32")

            # Generate fake test data (labeled as attack, 0)
            noise = tf.random.normal((len(X_test), self.latent_dim))
            fake_labels = tf.random.uniform((len(X_test),), minval=0, maxval=self.num_classes, dtype=tf.int32)
            generated_samples = self.generator.predict([noise, fake_labels])
            X_fake = generated_samples
            y_fake = np.zeros((len(X_test),), dtype="int32")

            # Compute custom NIDS loss on real and fake outputs
            real_output = self.nids.predict(X_real)
            fake_output = self.nids.predict(X_fake)
            custom_nids_loss = self.nids_loss(real_output, fake_output)

            # Combine real and fake data for evaluation
            X_combined = np.vstack([X_real, X_fake])
            y_combined = np.hstack([y_real, y_fake])
            nids_eval_results = self.nids.evaluate(X_combined, y_combined, verbose=0)
            # Expected order: [loss, accuracy, precision, recall, auc, logcosh]

            # Compute additional metrics
            y_pred_probs = self.nids.predict(X_combined)
            y_pred = (y_pred_probs > 0.5).astype("int32")
            f1 = f1_score(y_combined, y_pred)
            class_report = classification_report(
                y_combined, y_pred, target_names=["Attack (Fake)", "Benign (Real)"]
            )

            nids_eval_metrics = {
                "Custom NIDS Loss": f"{custom_nids_loss:.4f}",
                "Loss": f"{nids_eval_results[0]:.4f}",
                "Accuracy": f"{nids_eval_results[1]:.4f}",
                "Precision": f"{nids_eval_results[2]:.4f}",
                "Recall": f"{nids_eval_results[3]:.4f}",
                "AUC": f"{nids_eval_results[4]:.4f}",
                "LogCosh": f"{nids_eval_results[5]:.4f}",
                "F1 Score": f"{f1:.4f}"
            }
            self.logger.info(f"NIDS Custom Loss: {custom_nids_loss:.4f}")
            self.logger.info(
                f"NIDS Evaluation -> Loss: {nids_eval_results[0]:.4f}, Accuracy: {nids_eval_results[1]:.4f}, "
                f"Precision: {nids_eval_results[2]:.4f}, Recall: {nids_eval_results[3]:.4f}, "
                f"AUC: {nids_eval_results[4]:.4f}, LogCosh: {nids_eval_results[5]:.4f}")
            self.logger.info("NIDS Classification Report:")
            self.logger.info(class_report)

        # Log the overall evaluation metrics using our logging function
        self.log_evaluation_metrics(d_eval_metrics, g_eval_metrics, nids_eval_metrics)

    # -- Saving Models -- #
    def save(self, save_name):
        """
        Save trained models with information about the best epoch.

        Parameters:
        -----------
        save_name : str
            Base name to use for saving the models
        """
        # Save each submodel separately with best epoch info
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        best_epoch_info = f"_epoch{self.best_epoch}_acc{self.best_val_acc:.4f}"

        generator_path = f"../pretrainedModels/generator_local_ACGAN_{save_name}{best_epoch_info}.h5"
        discriminator_path = f"../pretrainedModels/discriminator_local_ACGAN_{save_name}{best_epoch_info}.h5"

        self.generator.save(generator_path)
        self.discriminator.save(discriminator_path)

        self.logger.info(f"Saved generator to: {generator_path}")
        self.logger.info(f"Saved discriminator to: {discriminator_path}")

        # Save training metadata
        metadata = {
            "timestamp": timestamp,
            "best_epoch": self.best_epoch,
            "best_accuracy": float(self.best_val_acc),
            "total_epochs": self.max_epochs,
            "epochs_trained": self.best_epoch + 1,
            "model_paths": {
                "generator": generator_path,
                "discriminator": discriminator_path
            }
        }

        metadata_path = f"../pretrainedModels/ACGAN_{save_name}_metadata.json"
        with open(metadata_path, 'w') as f:
            import json
            json.dump(metadata, f, indent=4)

        self.logger.info(f"Saved training metadata to: {metadata_path}")

        return {
            "generator_path": generator_path,
            "discriminator_path": discriminator_path,
            "metadata_path": metadata_path
        }