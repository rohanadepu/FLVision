#########################################################
#    Imports / Env setup                                #
#########################################################

import sys
import os
import random
from datetime import datetime
import argparse
sys.path.append(os.path.abspath('..'))
# TensorFlow & Flower
if 'TF_USE_LEGACY_KERAS' in os.environ:
    del os.environ['TF_USE_LEGACY_KERAS']
import flwr as fl
import tensorflow as tf
from tensorflow.keras.metrics import AUC, Precision, Recall
from tensorflow.keras.losses import LogCosh
from tensorflow.keras.optimizers import Adam
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
# other plugins
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from numpy import expand_dims
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
# import math
# import glob
# from tqdm import tqdm
# import seaborn as sns
# import pickle
# import joblib

from datasetHandling.datasetLoadProcess import datasetLoadProcess

from Client.overheadConfig.hyperparameterLoading import hyperparameterLoading
from Client.overheadConfig.modelCreateLoad import modelCreateLoad

# Load and Saving Configs
from hflGlobalModelTrainingConfig.ServerSaveOnlyConfig import SaveModelFedAvg
from hflGlobalModelTrainingConfig.ServerLoadOnlyConfig import LoadModelFedAvg
from hflGlobalModelTrainingConfig.ServerLoadNSaveConfig import LoadSaveModelFedAvg

# Fit on End configs
from hflGlobalModelTrainingConfig.ServerNIDSFitOnEndConfig import NIDSFitOnEndStrategy
# from hflGlobalModelTrainingConfig.ServerDiscBinaryFitOnEndConfig import DiscriminatorSyntheticStrategy
# from hflGlobalModelTrainingConfig.ServerWDiscFitOnEndConfig import WDiscriminatorSyntheticStrategy
# from hflGlobalModelTrainingConfig.ServerACDiscFitOnEndConfig import ACDiscriminatorSyntheticStrategy

################################################################################################################
#                                                   Execute                                                   #
################################################################################################################

def main():
    print("\n ////////////////////////////// \n")
    print("Centralized Training:", "\n")

    # Generate a static timestamp at the start of the script
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # --- 1. Argument Parsing --- #
    parser = argparse.ArgumentParser(description='Select dataset, model selection, and to enable DP respectively')

    # -- Dataset Settings -- #
    parser.add_argument('--dataset', type=str, choices=["CICIOT", "IOTBOTNET"], default="CICIOT",
                        help='Datasets to use: CICIOT, IOTBOTNET')
    parser.add_argument('--dataset_processing', type=str, choices=["Default", "MM[-1,-1]", "AC-GAN"], default="Default",
                        help='Datasets to use: Default, MM[-1,1], AC-GAN')

    # -- Server Hosting Modes -- #
    parser.add_argument('--serverLoad', action='store_true',
                        help='Only load the model structure and get the weights from the server')
    parser.add_argument('--serverSave', action='store_true',
                        help='Only load the model structure and get the weights from the server')
    parser.add_argument('--fitOnEnd', action='store_true',
                        help='Only load the model structure and get the weights from the server')

    # -- Training / Model Parameters -- #
    parser.add_argument('--model_type', type=str, choices=["NIDS", "GAN", "WGAN-GP", "AC-GAN"],
                        help='Please select NIDS ,GAN, WGAN-GP, or AC-GAN as the model type to train')
    parser.add_argument('--model_training', type=str, choices=["NIDS", "Discriminator", "GAN"],
                        help='Please select NIDS, Discriminator, GAN as the model type to train')

    # Optional
    parser.add_argument("--epochs", type=int, default=5, help="Number of epochs to train the model")

    parser.add_argument("--rounds", type=int, choices=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], default=1,
                        help="Rounds of training 1-10")

    parser.add_argument("--synth_portion", type=float, choices=[0, 0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6], default=0,
                       help="Percentage of Synthetic data compared to training data size to be augmented to that dataset, 0-0.6")

    parser.add_argument("--min_clients", type=int, choices=[1, 2, 3, 4, 5, 6], default=2,
                        help="Minimum number of clients required for training")

    # -- Loading Models Optional -- +
    parser.add_argument('--pretrained_GAN', type=str,
                        help="Path to pretrained discriminator model (optional)", default=None)
    parser.add_argument('--pretrained_generator', type=str, help="Path to pretrained generator model (optional)",
                        default=None)
    parser.add_argument('--pretrained_discriminator', type=str,
                        help="Path to pretrained discriminator model (optional)", default=None)
    parser.add_argument('--pretrained_nids', type=str,
                        help="Path to pretrained nids model (optional)", default=None)

    # -- Saving Models Optional -- +
    parser.add_argument('--save_name', type=str,
                        help="name of model files you save as", default=f"{timestamp}")

    # init variables to handle arguments
    args = parser.parse_args()

    # argument variables
    dataset_used = args.dataset
    dataset_processing = args.dataset_processing

    # Model Spec
    fitOnEnd = args.fitOnEnd
    serverSave = args.serverSave
    serverLoad = args.serverLoad
    model_type = args.model_type
    train_type = args.model_training
    if model_type == "AC-GAN":
        dataset_processing = "AC-GAN"
    if model_type == "NIDS":
        train_type = "NIDS"
    if train_type == "NIDS":
        model_type = "NIDS"

    # Training / Hyper Param
    epochs = args.epochs
    synth_portion = args.synth_portion
    regularizationEnabled = True
    DP_enabled = None
    earlyStopEnabled = None
    lrSchedRedEnabled = None
    modelCheckpointEnabled = None

    roundInput = args.rounds
    minClients = args.min_clients

    # Whole GAN model
    pretrainedGan = args.pretrained_GAN

    # Individual GAN Submodels
    pretrainedGenerator = args.pretrained_generator
    pretrainedDiscriminator = args.pretrained_discriminator

    # Optional NIDS
    pretrainedNids = args.pretrained_nids

    # Save/Record Param
    save_name_input = args.save_name
    evaluationLog = timestamp
    trainingLog = timestamp
    node = 1

    # Make Save name for model based on Arguments
    save_name = f""
    if fitOnEnd is True:
        save_name = f"'fitOnEnd'_{dataset_used}_{dataset_processing}_{model_type}_{train_type}_{save_name_input}.h5"
    # if base strategies
    else:
        save_name = f"{model_type}_{train_type}_{save_name_input}.h5"

    # -- Display selected arguments --#
    print("|MAIN CONFIG|", "\n")
    # main experiment config
    if fitOnEnd is True:
        print("Selected DATASET:", dataset_used, "\n")
        print("Selected Preprocessing:", dataset_processing, "\n")
        print("Selected Model Type:", model_type, "\n")
        print("Selected Model Training:", train_type, "\n")
        print("Selected Epochs:", epochs, "\n")
        print("Loaded GAN/GAN-Variant:", dataset_used, "\n")
    print("Loaded Generator Model:", pretrainedGenerator, "\n")
    print("Loaded Discriminator Model:", pretrainedDiscriminator, "\n")
    print("Loaded NIDS Model:", pretrainedNids, "\n")
    print("Save Name for the models in Trained in this session:", save_name, "\n")

    # -- 1B determine whether to do default federation hosting or do a custom strategy --#
    if serverLoad is False and serverSave is False and fitOnEnd is False:
        # --- Default, No Loading, No Saving ---#
        fl.server.start_server(
            config=fl.server.ServerConfig(num_rounds=roundInput),
            strategy=fl.server.strategy.FedAvg(
                min_fit_clients=minClients,
                min_evaluate_clients=minClients,
                min_available_clients=minClients
            )
        )

    # if the user wants to either load, save, or fit a model
    else:
        # --- 2 Load & Preprocess Data ---#
        X_train_data, X_val_data, y_train_data, y_val_data, X_test_data, y_test_data = datasetLoadProcess(dataset_used,
                                                                                                          dataset_processing)

        # --- 3 Model Hyperparameter & Training Parameters ---#
        (BATCH_SIZE, noise_dim, steps_per_epoch, input_dim, num_classes, latent_dim, betas, learning_rate, l2_alpha,
         l2_norm_clip, noise_multiplier, num_microbatches, metric_to_monitor_es, es_patience, restor_best_w,
         metric_to_monitor_l2lr, l2lr_patience, save_best_only,
         metric_to_monitor_mc, checkpoint_mode) = hyperparameterLoading(model_type, X_train_data,
                                                                        regularizationEnabled, DP_enabled,
                                                                        earlyStopEnabled,
                                                                        lrSchedRedEnabled, modelCheckpointEnabled)

        # --- 4 Model Loading & Creation ---#
        nids, discriminator, generator, GAN = modelCreateLoad(model_type, train_type, pretrainedNids, pretrainedGan,
                                                              pretrainedGenerator, pretrainedDiscriminator,
                                                              dataset_used,
                                                              input_dim, noise_dim, regularizationEnabled, DP_enabled,
                                                              l2_alpha, latent_dim, num_classes)
        # -- 5. run server based on selected config -- #
        # selet model for base hosting config
        if train_type == "GAN":
            model = GAN
        elif train_type == "Discriminator":
            model = discriminator
        else:
            model = nids

        # Non Fit on end Strats
        if fitOnEnd is False:
            # Server Load only Config
            if serverLoad is True and serverSave is False:
                # --- Load Model ---#
                fl.server.start_server(
                    config=fl.server.ServerConfig(num_rounds=roundInput),
                    strategy=LoadModelFedAvg(
                        model=model,
                        min_fit_clients=2,
                        min_evaluate_clients=2,
                        min_available_clients=2
                    )
                )

            # Server saving only
            elif serverLoad is False and serverSave is True:
                # --- Save Model ---#
                fl.server.start_server(
                    config=fl.server.ServerConfig(num_rounds=roundInput),
                    strategy=SaveModelFedAvg(
                        model=model,
                        model_save_path=save_name,
                        min_fit_clients=2,
                        min_evaluate_clients=2,
                        min_available_clients=2
                    )
                )

            # Server Load and Save model
            elif serverLoad is True and serverSave is True:
                # --- Load and Save Model ---#
                fl.server.start_server(
                    config=fl.server.ServerConfig(num_rounds=roundInput),
                    strategy=LoadSaveModelFedAvg(
                        model=model,
                        model_save_path=save_name,
                        min_fit_clients=2,
                        min_evaluate_clients=2,
                        min_available_clients=2
                    )
                )

        # If the server is fitting the global model at end of federation round
        else:
            # NIDS fit on end advanced synthetic training
            if train_type == "NIDS":
                fl.server.start_server(
                    config=fl.server.ServerConfig(num_rounds=roundInput),
                    strategy=NIDSFitOnEndStrategy(
                        discriminator=discriminator,  # Pre-trained or newly created discriminator
                        generator=generator,  # Pre-trained or newly created generator
                        nids=nids,
                        dataset_used=dataset_used,  # or "IOTBOTNET" depending on dataset
                        node=node,  # Server node identifier
                        earlyStopEnabled=earlyStopEnabled,  # Enable early stopping
                        DP_enabled=DP_enabled,  # Differential privacy enabled/disabled
                        X_train_data=X_train_data, y_train_data=y_train_data,  # Server-side training data
                        X_test_data=X_test_data, y_test_data=y_test_data,  # Test data
                        X_val_data=X_val_data, y_val_data=y_val_data,  # Validation data
                        l2_norm_clip=l2_norm_clip,  # L2 norm clipping for DP
                        noise_multiplier=noise_multiplier,  # Noise multiplier for DP
                        num_microbatches=num_microbatches,  # Microbatches for DP
                        batch_size=BATCH_SIZE,  # Training batch size
                        epochs=epochs,  # Number of fine-tuning epochs
                        steps_per_epoch=steps_per_epoch,  # Training steps per epoch
                        learning_rate=learning_rate,  # Optimizer learning rate
                        synth_portion=synth_portion,  # Portion of synthetic data used
                        latent_dim=latent_dim,
                        num_classes=num_classes,
                        metric_to_monitor_es=metric_to_monitor_es,  # Early stopping monitor metric
                        es_patience=es_patience,  # Early stopping patience
                        restor_best_w=restor_best_w,  # Restore best weights on early stopping
                        metric_to_monitor_l2lr=metric_to_monitor_l2lr,  # Learning rate schedule monitor
                        l2lr_patience=l2lr_patience,  # Learning rate schedule patience
                        save_best_only=save_best_only,  # Save best model only
                        metric_to_monitor_mc=metric_to_monitor_mc,  # Model checkpoint monitor metric
                        checkpoint_mode=checkpoint_mode, # Save best model based on max value of metric
                        save_name = save_name,
                        serverLoad = serverLoad,
                    )
                )
        #
        #     # fit on end discriminator from GAN models
        #     else:
        #         # load model ??
        #         # fit and save
        #
        #         # Discriminator advanced global synthetic training
        #         if model_type == "GAN":
        #             fl.server.start_server(
        #                 config=fl.server.ServerConfig(num_rounds=roundInput),
        #                 strategy=DiscriminatorSyntheticStrategy(
        #                     discriminator=discriminator,  # Pre-trained or newly created discriminator
        #                     generator=generator,  # Pre-trained or newly created generator
        #                     dataset_used="CICIOT",  # or "IOTBOTNET" depending on dataset
        #                     node="server_node_1",  # Server node identifier
        #                     adversarialTrainingEnabled=True,  # Enable adversarial training
        #                     earlyStopEnabled=True,  # Enable early stopping
        #                     DP_enabled=False,  # Differential privacy enabled/disabled
        #                     X_train_data=X_train_data, y_train_data=y_train_data,  # Server-side training data
        #                     X_test_data=X_test_data, y_test_data=y_test_data,  # Test data
        #                     X_val_data=X_val_data, y_val_data=y_val_data,  # Validation data
        #                     l2_norm_clip=1.0,  # L2 norm clipping for DP
        #                     noise_multiplier=0.1,  # Noise multiplier for DP
        #                     num_microbatches=32,  # Microbatches for DP
        #                     batch_size=32,  # Training batch size
        #                     epochs=10,  # Number of fine-tuning epochs
        #                     steps_per_epoch=100,  # Training steps per epoch
        #                     learning_rate=0.001,  # Optimizer learning rate
        #                     synth_portion=0.3,  # Portion of synthetic data used
        #                     adv_portion=0.3,  # Portion of adversarial data used
        #                     metric_to_monitor_es="val_loss",  # Early stopping monitor metric
        #                     es_patience=5,  # Early stopping patience
        #                     restor_best_w=True,  # Restore best weights on early stopping
        #                     metric_to_monitor_l2lr="val_loss",  # Learning rate schedule monitor
        #                     l2lr_patience=3,  # Learning rate schedule patience
        #                     save_best_only=True,  # Save best model only
        #                     metric_to_monitor_mc="val_accuracy",  # Model checkpoint monitor metric
        #                     checkpoint_mode="max"  # Save best model based on max value of metric
        #                 )
        #             )
        #
        #         # WGAN-GP Discriminator advanced global synthetic training
        #         elif model_type == "WGAN-GP":
        #             fl.server.start_server(
        #                 config=fl.server.ServerConfig(num_rounds=roundInput),
        #                 strategy=WDiscriminatorSyntheticStrategy(
        #                     discriminator=discriminator,  # Pre-trained or newly created discriminator
        #                     generator=generator,  # Pre-trained or newly created generator
        #                     dataset_used="CICIOT",  # or "IOTBOTNET" depending on dataset
        #                     node="server_node_1",  # Server node identifier
        #                     adversarialTrainingEnabled=True,  # Enable adversarial training
        #                     earlyStopEnabled=True,  # Enable early stopping
        #                     DP_enabled=False,  # Differential privacy enabled/disabled
        #                     X_train_data=X_train_data, y_train_data=y_train_data,  # Server-side training data
        #                     X_test_data=X_test_data, y_test_data=y_test_data,  # Test data
        #                     X_val_data=X_val_data, y_val_data=y_val_data,  # Validation data
        #                     l2_norm_clip=1.0,  # L2 norm clipping for DP
        #                     noise_multiplier=0.1,  # Noise multiplier for DP
        #                     num_microbatches=32,  # Microbatches for DP
        #                     batch_size=32,  # Training batch size
        #                     epochs=10,  # Number of fine-tuning epochs
        #                     steps_per_epoch=100,  # Training steps per epoch
        #                     learning_rate=0.001,  # Optimizer learning rate
        #                     synth_portion=0.3,  # Portion of synthetic data used
        #                     adv_portion=0.3,  # Portion of adversarial data used
        #                     metric_to_monitor_es="val_loss",  # Early stopping monitor metric
        #                     es_patience=5,  # Early stopping patience
        #                     restor_best_w=True,  # Restore best weights on early stopping
        #                     metric_to_monitor_l2lr="val_loss",  # Learning rate schedule monitor
        #                     l2lr_patience=3,  # Learning rate schedule patience
        #                     save_best_only=True,  # Save best model only
        #                     metric_to_monitor_mc="val_accuracy",  # Model checkpoint monitor metric
        #                     checkpoint_mode="max"  # Save best model based on max value of metric
        #                 )
        #             )
        #
        #         # AC Discriminator advanced global synthetic training
        #         elif model_type == "AC-GAN":
        #             fl.server.start_server(
        #                 config=fl.server.ServerConfig(num_rounds=roundInput),
        #                 strategy=ACDiscriminatorSyntheticStrategy(
        #                     discriminator=discriminator,  # Pre-trained or newly created discriminator
        #                     generator=generator,  # Pre-trained or newly created generator
        #                     dataset_used="CICIOT",  # or "IOTBOTNET" depending on dataset
        #                     node="server_node_1",  # Server node identifier
        #                     adversarialTrainingEnabled=True,  # Enable adversarial training
        #                     earlyStopEnabled=True,  # Enable early stopping
        #                     DP_enabled=False,  # Differential privacy enabled/disabled
        #                     X_train_data=X_train_data, y_train_data=y_train_data,  # Server-side training data
        #                     X_test_data=X_test_data, y_test_data=y_test_data,  # Test data
        #                     X_val_data=X_val_data, y_val_data=y_val_data,  # Validation data
        #                     l2_norm_clip=1.0,  # L2 norm clipping for DP
        #                     noise_multiplier=0.1,  # Noise multiplier for DP
        #                     num_microbatches=32,  # Microbatches for DP
        #                     batch_size=32,  # Training batch size
        #                     epochs=10,  # Number of fine-tuning epochs
        #                     steps_per_epoch=100,  # Training steps per epoch
        #                     learning_rate=0.001,  # Optimizer learning rate
        #                     synth_portion=0.3,  # Portion of synthetic data used
        #                     adv_portion=0.3,  # Portion of adversarial data used
        #                     metric_to_monitor_es="val_loss",  # Early stopping monitor metric
        #                     es_patience=5,  # Early stopping patience
        #                     restor_best_w=True,  # Restore best weights on early stopping
        #                     metric_to_monitor_l2lr="val_loss",  # Learning rate schedule monitor
        #                     l2lr_patience=3,  # Learning rate schedule patience
        #                     save_best_only=True,  # Save best model only
        #                     metric_to_monitor_mc="val_accuracy",  # Model checkpoint monitor metric
        #                     checkpoint_mode="max"  # Save best model based on max value of metric
        #                 )
        #             )

if __name__ == "__main__":
    main()